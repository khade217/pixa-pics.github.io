/*! For license information please see chunk_74.min.js.LICENSE.txt */
(window.webpackJsonp=window.webpackJsonp||[]).push([[74],{642:function(t,e,s){"use strict";function n(t,e,s){if((null==e||null!=s&&s>0)&&(e=t.sourceLayer,s=t.nodeIndex),0===e.inboundNodes.length)return[t];{const t=e.inboundNodes[s];if(0===t.inboundLayers.length)return t.inputTensors;{const e=[];for(let s=0;s<t.inboundLayers.length;s++){const i=n(t.inputTensors[s],t.inboundLayers[s],t.nodeIndices[s]);for(const t of i)-1===e.indexOf(t)&&e.push(t)}return e}}}var i,a,o,r,h,l,u,c,p;s.d(e,"a",(function(){return d})),s.d(e,"d",(function(){return f})),s.d(e,"c",(function(){return b})),s.d(e,"b",(function(){return y})),s.d(e,"e",(function(){return n})),i=s(616),a=s(816),o=s(691),r=s(633),h=s(700),l=s(648),u=s(654),c=s(1205),p=s(817);class d{constructor(t){this.dtype=t.dtype,this.shape=t.shape,null!=t.shape?this.ndim=t.shape.length:this.ndim=t.ndim,this.maxNDim=t.maxNDim,this.minNDim=t.minNDim,this.axes=t.axes||{}}}class f{constructor(t,e,s,n,i,r,h){this.dtype=t,this.shape=e,this.sourceLayer=s,this.inputs=n,this.callArgs=i,this.outputTensorIndex=h,this.id=Object(a.a)(),null!=r&&(this.originalName=Object(o.e)(r),this.name=Object(o.f)(this.originalName)),this.rank=e.length}}let g=0;class b{constructor(t,e){this.callArgs=e,this.id=g++,this.outboundLayer=t.outboundLayer,this.inboundLayers=t.inboundLayers,this.nodeIndices=t.nodeIndices,this.tensorIndices=t.tensorIndices,this.inputTensors=t.inputTensors,this.outputTensors=t.outputTensors,this.inputMasks=t.inputMasks,this.outputMasks=t.outputMasks,this.inputShapes=t.inputShapes,this.outputShapes=t.outputShapes;for(const s of t.inboundLayers)null!=s&&s.outboundNodes.push(this);t.outboundLayer.inboundNodes.push(this)}getConfig(){const t=[];for(const e of this.inboundLayers)null!=e?t.push(e.name):t.push(null);return{outboundLayer:this.outboundLayer?this.outboundLayer.name:null,inboundLayers:t,nodeIndices:this.nodeIndices,tensorIndices:this.tensorIndices}}}let m=0;class y extends i.Lh.Serializable{constructor(t={}){super(),this._callHook=null,this._addedWeightNames=[],this._stateful=!1,this.id=m++,this.activityRegularizer=null,this.inputSpec=null,this.supportsMasking=!1,this._trainableWeights=[],this._nonTrainableWeights=[],this._losses=[],this._updates=[],this._built=!1,this.inboundNodes=[],this.outboundNodes=[];let e=t.name;if(!e){const t=this.getClassName();e=l.p(t)+"_"+Object(a.b)(t)}if(this.name=e,this.trainable_=null==t.trainable||t.trainable,null!=t.inputShape||null!=t.batchInputShape){let e;if(null!=t.batchInputShape)e=t.batchInputShape;else if(null!=t.inputShape){let s=null;null!=t.batchSize&&(s=t.batchSize),e=[s].concat(t.inputShape)}this.batchInputShape=e;let s=t.dtype;null==s&&(s=t.inputDType),null==s&&(s="float32"),this.dtype=s}null!=t.weights?this.initialWeights=t.weights:this.initialWeights=null,this._refCount=null,this.fastWeightInitDuringBuild=!1}static nodeKey(t,e){return t.name+"_ib-"+e.toString()}getNodeAtIndex(t,e){if(0===this.inboundNodes.length)throw new r.d(`The layer has never been called and thus has no defined ${e}.`);if(this.inboundNodes.length<=t)throw new r.e(`Asked to get ${e} at node ${t}, but the layer has only ${this.inboundNodes.length} inbound nodes.`);return this.inboundNodes[t]}getInputAt(t){return l.m(this.getNodeAtIndex(t,"input").inputTensors)}getOutputAt(t){return l.m(this.getNodeAtIndex(t,"output").outputTensors)}get input(){if(this.inboundNodes.length>1)throw new r.b("Layer "+this.name+' has multiple inbound nodes, hence the notion of "layer input" is ill-defined. Use `getInputAt(nodeIndex)` instead.');if(0===this.inboundNodes.length)throw new r.b("Layer "+this.name+" is not connected, no input to return.");return l.m(this.getNodeAtIndex(0,"input").inputTensors)}get output(){if(0===this.inboundNodes.length)throw new r.b("Layer "+this.name+" has no inbound nodes.");if(this.inboundNodes.length>1)throw new r.b("Layer "+this.name+' has multiple inbound nodes, hence the notion of "layer output" is ill-defined. Use `getOutputAt(nodeIndex)` instead.');return l.m(this.getNodeAtIndex(0,"output").outputTensors)}get losses(){return this._losses}calculateLosses(){return this.losses.map((t=>t()))}get updates(){return this._updates}get built(){return this._built}set built(t){this._built=t}get trainable(){return this.trainable_}set trainable(t){this._trainableWeights.forEach((e=>e.trainable=t)),this.trainable_=t}get trainableWeights(){return this.trainable_?this._trainableWeights.filter((t=>t.trainable)):[]}set trainableWeights(t){this._trainableWeights=t}get nonTrainableWeights(){return this.trainable?this._trainableWeights.filter((t=>!t.trainable)).concat(this._nonTrainableWeights):this._trainableWeights.concat(this._nonTrainableWeights)}set nonTrainableWeights(t){this._nonTrainableWeights=t}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}get stateful(){return this._stateful}resetStates(){if(!this.stateful)throw Error("Cannot call the resetStates() method of a non-stateful Layer object.")}assertInputCompatibility(t){const e=l.o(t);if(null==this.inputSpec||0===this.inputSpec.length)return;const s=l.o(this.inputSpec);if(e.length!==s.length)throw new r.e(`Layer ${this.name} expects ${s.length} inputs, but it received ${e.length} input tensors. Input received: `+t);for(let n=0;n<e.length;n++){const t=e[n],i=s[n];if(null==i)continue;const a=t.rank;if(null!=i.ndim&&a!==i.ndim)throw new r.e(`Input ${n} is incompatible with layer ${this.name}: expected ndim=${i.ndim}, found ndim=${a}`);if(null!=i.maxNDim&&a>i.maxNDim)throw new r.e(`Input ${n} is incompatible with layer ${this.name}: expected max_ndim=${i.maxNDim}, found ndim=${a}`);if(null!=i.minNDim&&a<i.minNDim)throw new r.e(`Input ${n} is incompatible with layer ${this.name}: expected min_ndim=${i.minNDim}, found ndim=${a}.`);if(null!=i.dtype&&t.dtype!==i.dtype)throw new r.e(`Input ${n} is incompatible with layer ${this.name} : expected dtype=${i.dtype}, found dtype=${t.dtype}.`);if(i.axes){const e=t.shape;for(const t in i.axes){const s=Number(t),a=i.axes[t],o=s>=0?e[s]:e[e.length+s];if(null!=a&&-1===[a,null].indexOf(o))throw new r.e(`Input ${n} is incompatible with layer ${this.name}: expected axis ${s} of input shape to have value ${a} but got shape ${e}.`)}}if(null!=i.shape)for(let e=0;e<i.shape.length;++e){const s=i.shape[e],a=t.shape[e];if(null!=s&&null!=a&&s!==a)throw new r.e(`Input ${n} is incompatible with layer ${this.name}: expected shape=${i.shape}, found shape=${t.shape}.`)}}}call(t,e){return t}invokeCallHook(t,e){null!=this._callHook&&this._callHook(t,e)}setCallHook(t){this._callHook=t}clearCallHook(){this._callHook=null}apply(t,e){e=e||{},this.assertNotDisposed();const s=l.o(t),n=function(t){let e=!0;for(const s of l.o(t))if(!(s instanceof f)){e=!1;break}return e}(t),i=function(t){let e=!0;for(const s of l.o(t))if(s instanceof f){e=!1;break}return e}(t);if(n===i)throw new r.e("Arguments to apply() must be all SymbolicTensors or all Tensors");return Object(o.g)(this.name,(()=>{if(!this.built){this.assertInputCompatibility(t);const e=[];for(const s of l.o(t))e.push(s.shape);this.build(l.m(e)),this.built=!0,this.initialWeights&&this.setWeights(this.initialWeights),null===this._refCount&&i&&(this._refCount=1)}if(this.assertInputCompatibility(t),i){let n=this.call(t,e);this.supportsMasking&&this.setMaskMetadata(t,n);const i=l.o(n),a=[];for(let t of i)-1!==s.indexOf(t)&&(t=t.clone()),a.push(t);if(n=l.m(a),null!=this.activityRegularizer)throw new r.c("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return n}{const s=function(t){t=l.o(t);const e=[];for(const s of t)e.push(s.shape);return l.m(e)}(t),n=this.computeOutputShape(s);let i;const a="float32";if(this.warnOnIncompatibleInputShape(Array.isArray(t)?s[0]:s),i=null!=n&&n.length>0&&Array.isArray(n[0])?n.map(((s,n)=>new f(a,s,this,l.o(t),e,this.name,n))):new f(a,n,this,l.o(t),e,this.name),this.addInboundNode(t,i,null,null,s,n,e),this._refCount++,null!=this.activityRegularizer)throw new r.c("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return i}}))}warnOnIncompatibleInputShape(t){if(null!=this.batchInputShape)if(t.length!==this.batchInputShape.length)console.warn("The rank of the input tensor provided (shape: "+JSON.stringify(t)+") does not match that of the "+`batchInputShape (${JSON.stringify(this.batchInputShape)}) of the layer `+this.name);else{let e=!1;this.batchInputShape.forEach(((s,n)=>{null!=s&&null!=t[n]&&t[n]!==s&&(e=!0)})),e&&console.warn(`The shape of the input tensor (${JSON.stringify(t)}) does not match the expectation of layer ${this.name}: `+JSON.stringify(this.batchInputShape))}}get outputShape(){if(null==this.inboundNodes||0===this.inboundNodes.length)throw new r.b(`The layer ${this.name} has never been called and thus has no defined output shape.`);const t=[];for(const e of this.inboundNodes){const s=JSON.stringify(e.outputShapes);-1===t.indexOf(s)&&t.push(s)}if(1===t.length){const t=this.inboundNodes[0].outputShapes;return Array.isArray(t)&&Array.isArray(t[0])&&1===t.length?t[0]:t}throw new r.b(`The layer ${this.name} has multiple inbound nodes with different output shapes. Hence the notion of "output shape" is ill-defined for the layer.`)}countParams(){if(!this.built)throw new r.d(`You tried to call countParams() on ${this.name}, but the layer is not built yet. Build it first by calling build(batchInputShape).`);return c.a(this.weights)}build(t){this.built=!0}getWeights(t=!1){return Object(p.b)(t?this.trainableWeights:this.weights)}setWeights(t){Object(i.Ei)((()=>{const e=this.weights;if(e.length!==t.length)throw new r.e(`You called setWeights(weights) on layer "${this.name}" with a weight list of length ${t.length}, but the layer was expecting ${e.length} weights. Provided weights: ${t}...`);if(0===e.length)return;const s=[],n=Object(p.b)(e);for(let a=0;a<n.length;++a){const o=n[a],h=e[a],l=t[a];if(!i.Si.arraysEqual(o.shape,l.shape))throw new r.e(`Layer weight shape ${o.shape} not compatible with provided weight shape `+l.shape);s.push([h,l])}Object(p.c)(s)}))}addWeight(t,e,s,n,i,a,o,l){if(-1!==this._addedWeightNames.indexOf(t))throw new r.e(`Duplicate weight name ${t} for layer ${this.name}`);this._addedWeightNames.push(t),null==s&&(s="float32"),this.fastWeightInitDuringBuild&&(n=null!=l?l():Object(h.q)("zeros"));const u=n.apply(e,s),c=new p.a(u,s,t,a,o);return u.dispose(),null!=i&&this.addLoss((()=>i.apply(c.read()))),null==a&&(a=!0),a?this._trainableWeights.push(c):this._nonTrainableWeights.push(c),c}setFastWeightInitDuringBuild(t){this.fastWeightInitDuringBuild=t}addLoss(t){null==t||Array.isArray(t)&&0===t.length||(t=l.o(t),void 0!==this._losses&&null!==this._losses&&this.losses.push(...t))}computeOutputShape(t){return t}computeMask(t,e){if(!this.supportsMasking){if(null!=e){if(!Array.isArray(e))throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`);e.forEach((t=>{if(null!=t)throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`)}))}return null}return e}setMaskMetadata(t,e,s){if(!this.supportsMasking)return;const n=this.computeMask(t,s),i=l.o(e),a=l.o(n);if(i.length!==a.length)throw Error(`${this.name} outputs ${i.length} tensors but ${i.length} masks for those tensors`);for(let o=0;o<i.length;o++)i[o].kerasMask=a[o]}addInboundNode(t,e,s,n,i,a,o=null){const r=l.o(t);e=l.o(e),s=l.o(s),n=l.o(n),i=u.d(i),a=u.d(a);const h=[],c=[],p=[];for(const l of r)h.push(l.sourceLayer),c.push(l.nodeIndex),p.push(l.tensorIndex);new b({outboundLayer:this,inboundLayers:h,nodeIndices:c,tensorIndices:p,inputTensors:r,outputTensors:e,inputMasks:s,outputMasks:n,inputShapes:i,outputShapes:a},o);for(let l=0;l<e.length;l++)e[l].sourceLayer=this,e[l].nodeIndex=this.inboundNodes.length-1,e[l].tensorIndex=l}getConfig(){const t={name:this.name,trainable:this.trainable};return null!=this.batchInputShape&&(t.batchInputShape=this.batchInputShape),null!=this.dtype&&(t.dtype=this.dtype),t}disposeWeights(){return this.weights.forEach((t=>t.dispose())),this.weights.length}assertNotDisposed(){if(0===this._refCount)throw Error(`Layer '${this.name}' is already disposed.`)}dispose(){if(!this.built)throw Error(`Cannot dispose Layer ${this.name} because it has not been built yet.`);if(null===this._refCount)throw Error(`Cannot dispose Layer ${this.name} because it has not been used yet.`);this.assertNotDisposed();let t=0;return 0==--this._refCount&&(t=this.disposeWeights()),{refCountAfterDispose:this._refCount,numDisposedVariables:t}}}},815:function(t,e,s){"use strict";function n(t){if(null==t.batchShape&&null==t.shape)throw Error("Please provide to Input either a `shape` or a `batchShape` argument. Note that `shape` does not include the batch dimension.");if(null!=t.batchShape&&null!=t.shape)throw new o.e("Please provide either a `shape` or `batchShape` argument to Input, but not both.");let e=t.batchShape;null!=t.shape&&null==e&&(e=[null].concat(t.shape));let s=t.dtype;return null==s&&(s="float32"),new h({batchInputShape:e,name:t.name,dtype:s,sparse:t.sparse}).inboundNodes[0].outputTensors[0]}var i,a,o,r;s.d(e,"b",(function(){return h})),s.d(e,"a",(function(){return n})),i=s(616),a=s(816),o=s(633),r=s(642);class h extends r.b{constructor(t){if(super({dtype:t.dtype,name:null!=t.name?t.name:Object(a.b)("input").toString()}),null==t.batchSize&&(t.batchSize=null),null==t.sparse&&(t.sparse=!1),this.trainable=!1,this.built=!0,this.sparse=t.sparse,null!=t.inputShape&&null!=t.batchInputShape)throw new o.e("Only provide the inputShape OR batchInputShape argument to inputLayer, not both at the same time.");let e=t.batchInputShape;if(null==e){if(null==t.inputShape)throw new o.e("An InputLayer should be passed either a `batchInputShape` or an `inputShape`.");e=[t.batchSize].concat(t.inputShape)}else if(null!=t.batchSize)throw new o.e("Cannot specify batchSize if batchInputShape is specified when creating an InputLayer.");const s=t.dtype||"float32";this.batchInputShape=e,this.dtype=s,this.inputSpec=[{shape:e}];const n=new r.d(this.dtype,this.batchInputShape,this,[],{},this.name);n.nodeIndex=0,n.tensorIndex=0,new r.c({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:[n],outputTensors:[n],inputMasks:[null],outputMasks:[null],inputShapes:[e],outputShapes:[e]})}apply(t,e){throw new o.e("Cannot pass any input to an InputLayer's apply() method. InputLayer name: "+this.name)}dispose(){return{refCountAfterDispose:this._refCount,numDisposedVariables:0}}getConfig(){return{batchInputShape:this.batchInputShape,dtype:this.dtype,sparse:this.sparse,name:this.name}}}h.className="InputLayer",i.Lh.registerClass(h)},831:function(t,e,s){"use strict";function n(t,e){return function(t,e,s){const n=e.length;if(null==t||Array.isArray(t)&&0===t.length)return e.map((t=>null));if(1===n)return Array.isArray(t)&&1===t.length?t:"object"==typeof t&&e[0]in t?[t[e[0]]]:[t];if(Array.isArray(t)){if(t.length!==n)throw Error(`Provided ${s} is an array of ${t.length} element(s), but the model has ${n} outputs. Make sure a set of weights is provided for each model output.`);return t}if("object"==typeof t&&Object.keys(t).length>0&&"object"==typeof t[Object.keys(t)[0]]){const s=[];return e.forEach((e=>{e in t?s.push(t[e]):s.push(null)})),s}throw Error(`The model has multiple (${n}) outputs, so ${s} must be either an array with ${n} elements or an object with ${e} keys. Provided ${s} not understood: ${JSON.stringify(t)}`)}(t,e,"classWeight")}async function i(t,e,s,n){if(null!=e||null!=n)throw Error("Support sampleWeight is not implemented yet");if(null!=s){const e=Object(w.Ei)((()=>{if(1===t.shape.length)return Object(w.Ae)(t);if(2===t.shape.length){if(t.shape[1]>1){const e=1;return Object(w.Yd)(t,e)}if(1===t.shape[1])return Object(w.wh)(t,[t.shape[0]]);throw Error(`Encountered unexpected last-dimension size (${t.shape[1]}) during handling of class weights. The size is expected to be >= 1.`)}throw Error(`Unexpected rank of target (y) tensor (${t.rank}) during handling of class weights. The rank is expected to be 1 or 2.`)})),n=Array.from(await e.data());Object(w.bf)(e);const i=[];return n.forEach((t=>{if(null==s[t])throw Error(`classWeight must contain all classes in the training data. The class ${t} exists in the data but not in classWeight`);i.push(s[t])})),Object(w.vi)(i,"float32")}return null}function a(t,e){return Object(w.Gg)(t,e)}function o(t,e){let s,n;const i=e;s=i.xs,n=i.ys,w.Si.assert(null!=s&&null!=n,(()=>"A Dataset iterator for fitDataset() is expected to generate objects of the form `{xs: xVal, ys: yVal}`, where the two values may be `tf.Tensor`, an array of Tensors, or a map of string to Tensor.  The provided Dataset instead generates "+e));const a=r("input",t.inputNames,s),o=r("output",t.outputNames,n),h=a[0].shape[0];w.Si.assert(a.length===t.inputs.length,(()=>`LayersModel has ${t.inputs.length} inputs, but the dataset provides ${a.length} inputs.  (Expected input keys: `+JSON.stringify(t.inputNames)+")")),w.Si.assert(o.length===t.outputs.length,(()=>`LayersModel has ${t.outputs.length} outputs, but the dataset provides ${o.length} outputs.  (Expected output keys: `+JSON.stringify(t.outputNames)+")"));for(let r=0;r<a.length;r++)w.Si.assert(a[r].shape[0]===h,(()=>`Batch size mismatch: input ${t.inputNames[r]} has ${a[r].shape[0]}; expected  ${h} based on input ${t.inputNames[0]}.`));for(let r=0;r<o.length;r++)w.Si.assert(o[r].shape[0]===h,(()=>`Batch size mismatch: output ${t.outputNames[r]} has ${o[r].shape[0]}; expected  ${h} based on input ${t.inputNames[0]}.`));return{xs:a,ys:o}}function r(t,e,s){if(s instanceof w.Dd)return[s];if(Array.isArray(s))return w.Si.assert(s.length===e.length,(()=>`Received an array of ${s.length} Tensors, but expected ${e.length} to match the ${t} keys ${e}.`)),s;{const n=[];for(const i of e){if(null==s[i])throw new $.e(`The feature data generated by the dataset lacks the required ${t} key '${i}'.`);n.push(s[i])}return n}}function h(t){return"function"==typeof t.iterator}function l(t){w.Si.assert(t>0&&Number.isInteger(t),(()=>"batchSize is required to be a positive integer, but got "+t))}function u(t,e,s){return null==t?[null]:Array.isArray(t)?t.map((t=>Object(S.q)(t,e,s-e))):Object(S.q)(t,e,s-e)}function c(t,e){return w.Ei((()=>null==t?null:Array.isArray(t)?t.map((t=>c(t,e))):Object(S.k)(t,"int32"===e.dtype?e:w.xe(e,"int32"))))}function p(t,e){const s=[];let n=0,i=null;for(;n<t;)i=n+e,i>=t&&(i=t),s.push([n,i]),n=i;return s}function d(t){const e=[];t instanceof w.Dd&&(t=[t]);for(let s=0;s<t.length;++s){const n=t[s];if(1===n.rank)e.push(Object(S.i)(n,1));else{if(0===n.rank)throw Error("Expected tensor to be at least 1D, but received a 0D tensor (scalar).");e.push(n)}}return e}function f(t,e){if(null==t)return;const s=[];if(e instanceof w.Dd)s.push(e.id);else if(Array.isArray(e))e.forEach((t=>s.push(t.id)));else if(null!=e)for(const i in e){const t=e[i];s.push(t.id)}const n=[];if(t instanceof w.Dd)-1===s.indexOf(t.id)&&n.push(t);else if(Array.isArray(t))t.forEach((t=>{-1===s.indexOf(t.id)&&n.push(t)}));else if(null!=t)for(const i in t){const e=t[i];-1===s.indexOf(e.id)&&n.push(e)}n.forEach((t=>{t.isDisposed||t.dispose()}))}function g(t){return Array.isArray(t)}function b(t){return!function(t){return t instanceof w.Dd}(t)&&!g(t)}function m(t,e,s,n=!0,i=""){if(null==e||0===e.length){if(null!=t){let e=!1;if(g(t)&&t.length>0)e=!0;else if(b(t)){for(const s in t)if(t.hasOwnProperty(s)){e=!0;break}}else e=!0;if(e)throw new $.e(`Error when checking model ${i} expected no data, but got `+t)}return[]}if(null==t)return e.map((t=>null));let a;if(b(t)){a=[];for(const s of e){if(null==t[s])throw new $.e(`No data provided for "${s}". Need data for each key in: `+e);a.push(t[s])}}else if(g(t)){if(t.length!==e.length)throw new $.e(`Error when checking model ${i}: the Array of Tensors that you are passing to your model is not the size the model expected. Expected to see ${e.length} Tensor(s), but instead got the following list of Tensor(s): `+t);a=t}else{if(e.length>1)throw new $.e(`The model ${i} expects ${e.length} Tensor(s), but only received one Tensor. Found: Tensor with shape `+t.shape);a=[t]}if(a=d(a),null!=s)for(let o=0;o<e.length;++o){if(null==s[o])continue;const t=a[o];if(t.shape.length!==s[o].length)throw new $.e(`Error when checking ${i}: expected ${e[o]} to have ${s[o].length} dimension(s). but got array with shape `+t.shape);for(let e=0;e<s[o].length;++e){if(0===e&&!n)continue;const a=t.shape[e],r=s[o][e];if(null!=r&&r>=0&&a!==r)throw new $.e(i+" expected a batch of elements where each "+`example has shape [${s[o].slice(1,s[o].length)}] `+`(i.e.,tensor shape [*,${s[o].slice(1,s[o].length)}])`+` but the ${i} received an input with ${t.shape[0]}`+` examples, each with shape [${t.shape.slice(1,t.shape.length)}]`+` (tensor shape [${t.shape}])`)}}return a}function y(t,e,s,n=!0,i=""){let a;if(Array.isArray(t)){if(t.length!==e.length)throw new $.e(`Error when checking model ${i}: the Array of Tensors that you are passing to your model is not the size the the model expected. Expected to see ${e.length} Tensor(s), but instead got ${t.length} Tensors(s).`);a=t}else{if(e.length>1)throw new $.e(`The model expects ${e.length} ${i} Tensors, but only received one Tensor. Found: array with shape `+JSON.stringify(t.shape)+".");a=[t]}if(null!=s)for(let o=0;o<e.length;++o){if(null==s[o])continue;const t=a[o];if(t.shape.length!==s[o].length)throw new $.e(`Error when checking ${i}: expected ${e[o]} to have ${s[o].length} dimension(s), but got array with shape `+JSON.stringify(t.shape));for(let a=0;a<s[o].length;++a){if(0===a&&!n)continue;const r=t.shape[a],h=s[o][a];if(null!=h&&h!==r)throw new $.e(`Error when checking ${i}: expected ${e[o]} to have shape ${JSON.stringify(s[o])} but got array with shape ${JSON.stringify(t.shape)}.`)}}}var w=s(616),S=s(659),O=s(726),v=s(691),$=s(633),T=s(818),N=s(885),x=s(779),k=s(1206),I=s(1744),A=s(1745),E=s(648),_=s(1746),j=s(723),D=s(983),z=s(886),L=s(1747),W=s(982);s.d(e,"a",(function(){return C}));class C extends L.a{constructor(t){super(t),this.isTraining=!1}summary(t,e,s=console.log){if(!this.built)throw new $.e("This model has never been called, thus its weights have not been created yet. So no summary can be displayed. Build the model first (e.g., by calling it on some test data).");Object(_.a)(this,t,e,s)}compile(t){if(null==t.loss&&(t.loss=[]),this.loss=t.loss,"string"==typeof t.optimizer)this.optimizer_=I.a(t.optimizer),this.isOptimizerOwned=!0;else{if(!(t.optimizer instanceof w.uc))throw new $.e("User-defined optimizer must be an instance of tf.Optimizer.");this.optimizer_=t.optimizer,this.isOptimizerOwned=!1}let e=[];if(Array.isArray(t.loss)||"string"==typeof t.loss||"function"==typeof t.loss)if(Array.isArray(t.loss)){if(t.loss.length!==this.outputs.length)throw new $.e(`When passing an Array as loss, it should have one entry per model output. The model has ${this.outputs.length} output(s), but you passed loss=${t.loss}.`);const s=t.loss;e=s.map((t=>x.d(t)))}else{const s=x.d(t.loss);this.outputs.forEach((t=>{e.push(s)}))}else{t.loss=t.loss;for(const e in t.loss)if(-1===this.outputNames.indexOf(e))throw new $.e(`Unknown entry in loss dictionary: "${e}". Only expected the following keys: `+this.outputNames);for(const s of this.outputNames)null==t.loss[s]&&console.warn(`Output "${s}" is missing from loss dictionary. We assume this was done on purpose, and we will not be expecting data to be passed to ${s} during training`),e.push(x.d(t.loss[s]))}this.lossFunctions=e,this.feedOutputNames=[],this.feedOutputShapes=[],this.feedLossFns=[];for(let a=0;a<this.outputs.length;++a){const t=this.internalOutputShapes[a],e=this.outputNames[a];this.feedOutputNames.push(e),this.feedOutputShapes.push(t),this.feedLossFns.push(this.lossFunctions[a])}const s=[];this.metrics=t.metrics,this.metricsNames=["loss"],this.metricsTensors=[],Object(v.g)("loss",(()=>{for(let t=0;t<this.outputs.length;++t){if(-1!==s.indexOf(t))continue;const e=this.lossFunctions[t];this.outputs.length>1&&(this.metricsTensors.push([e,t]),this.metricsNames.push(this.outputNames[t]+"_loss"))}}));const n=function(t,e){if(null==t||Array.isArray(t)&&0===t.length)return e.map((t=>[]));let s;if("string"==typeof t||"function"==typeof t)s=[t];else{if(!Array.isArray(t)&&"object"!=typeof t)throw new TypeError("Type of metrics argument not understood. Expected an string,function, Array, or Object, found: "+t);s=t}if(Array.isArray(s))return e.map((t=>s));{const t=[];for(const n of e){let e=s.hasOwnProperty(n)?s[n]:[];Array.isArray(e)||(e=[e]),t.push(e)}return t}}(t.metrics,this.outputNames),i=(t,e,s)=>{this.outputNames.length>1&&(e=this.outputNames[t]+"_"+e),this.metricsNames.push(e),this.metricsTensors.push([s,t])};Object(v.g)("metric",(()=>{for(let t=0;t<this.outputs.length;++t)-1===s.indexOf(t)&&(e=>{let s,n,a;for(const o of e){if("string"==typeof o&&-1!==["accuracy","acc","crossentropy","ce"].indexOf(o)){const e=this.internalOutputShapes[t];let i;1===e[e.length-1]||this.lossFunctions[t]===x.a?-1!==["accuracy","acc"].indexOf(o)?n=k.a:-1!==["crossentropy","ce"].indexOf(o)&&(n=k.b):this.lossFunctions[t]===x.j?-1!==["accuracy","acc"].indexOf(o)?n=k.i:-1!==["crossentropy","ce"].indexOf(o)&&(n=k.j):-1!==["accuracy","acc"].indexOf(o)?n=k.c:-1!==["crossentropy","ce"].indexOf(o)&&(n=k.d),-1!==["accuracy","acc"].indexOf(o)?i="acc":-1!==["crossentropy","ce"].indexOf(o)&&(i="ce"),a=n,s=""+i}else{const t=k.e(o);a=t,s=""+k.f(o)}let e;Object(v.g)(s,(()=>{e=a})),i(t,s,e)}})(n[t])})),this.collectedTrainableWeights=this.trainableWeights}checkTrainableWeightsConsistency(){null!=this.collectedTrainableWeights&&this.trainableWeights.length!==this.collectedTrainableWeights.length&&console.warn("Discrepancy between trainableweights and collected trainable weights. Did you set `model.trainable` without calling `model.compile()` afterwards?")}evaluate(t,e,s={}){const n=null==s.batchSize?32:s.batchSize;l(n);const i=this.standardizeUserDataXY(t,e,!0,n);try{const t=i[0].concat(i[1]);this.makeTestFunction();const e=this.testFunction,a=this.testLoop(e,t,n,s.verbose,s.steps);return Object(E.m)(a)}finally{f(i[0],t),f(i[1],e)}}async evaluateDataset(t,e){return this.makeTestFunction(),async function(t,e,s){const n=null!=(s=s||{}).batches,i=t.testFunction;let a=[];if(s.verbose>0)throw new $.c("Verbose mode is not implemented yet.");w.Si.assert(!n||s.batches>0&&Number.isInteger(s.batches),(()=>"Test loop expects `batches` to be a positive integer, but received "+JSON.stringify(s.batches)));const r="function"==typeof e.next?e:await e.iterator();let h=0,l=0;for(;!n||l<s.batches;){const e=await r.next();if(a=w.Ei((()=>{if(e.value){const{xs:s,ys:n}=o(t,e.value),r=s.concat(n),u=w.Ei((()=>i(r)));if(w.bf(r),0===l)for(let t=0;t<u.length;++t)a.push(Object(w.Fh)(0));const c=r[0].shape[0];for(let t=0;t<u.length;++t){const e=u[t],s=a[t];a[t]=w.Ei((()=>w.Ud(a[t],w.Gg(c,e)))),l>0&&w.bf(s)}w.bf(u),h+=c,++l}return a})),e.done){n&&console.warn(`Your dataset iterator ran out of data during evaluateDataset(). Interrupting evalution. Make sure that your dataset can generate at least \`batches\` batches (in this case, ${s.batches} batches). You may need to use the repeat() function when building your dataset.`);break}}for(let o=0;o<a.length;++o){const t=a[o];a[o]=w.df(a[o],h),w.bf(t)}return Object(E.m)(a)}(this,t,e)}checkNumSamples(t,e,s,n="steps"){let i;if(null!=s){if(i=null,null!=e)throw new $.e(`If ${n} is set, batchSize must be null or undefined.Got batchSize = `+e)}else{if(null==t)throw new $.e("Either the input data should have a defined shape, or "+n+" shoud be specified.");i=Array.isArray(t)?t[0].shape[0]:t.shape[0]}return i}execute(t,e){if(Array.isArray(e)&&0===e.length)throw new $.e("`outputs` is an empty Array, which is not allowed.");const s=Array.isArray(e),n=s?e:[e],i=this.retrieveSymbolicTensors(n),a=new W.a;if(t instanceof w.Dd&&(t=[t]),Array.isArray(t)){if(t.length!==this.inputs.length)throw new $.e(`The number of inputs provided (${t.length}) does not match the number of inputs of this model (${this.inputs.length}).`);for(let e=0;e<this.inputs.length;++e)a.add(this.inputs[e],t[e])}else for(const r of this.inputs){const e=t[r.name];if(null==e)throw new $.e("No value is provided for the model's input "+r.name);a.add(r,e)}const o=Object(W.b)(i,a);return s?o:o[0]}retrieveSymbolicTensors(t){const e=Object(E.j)(null,t.length);let s=t.length;for(const n of this.layers){const i=Array.isArray(n.output)?n.output:[n.output],a=i.map((t=>t.name));for(let n=0;n<t.length;++n){const o=a.indexOf(t[n]);if(-1!==o&&(e[n]=i[o],s--),0===s)break}if(0===s)break}if(s>0){const s=[];throw e.forEach(((e,n)=>{null==e&&s.push(t[n])})),new $.e("Cannot find SymbolicTensors for output name(s): "+JSON.stringify(s))}return e}predictLoop(t,e=32,s=!1){return w.Ei((()=>{const n=this.checkNumSamples(t);if(s)throw new $.c("Verbose predictLoop() is not implemented yet.");const i=p(n,e),a=this.outputs.map((t=>[]));for(let e=0;e<i.length;++e)w.Ei((()=>{const s=i[e][0],n=i[e][1],a=u(t,s,n),o=[];if(Array.isArray(a))for(let t=0;t<a.length;++t)o.push({key:this.inputs[t],value:a[t]});else o.push({key:this.inputs[0],value:a});const r=new W.a(o);return Object(W.b)(this.outputs,r)})).forEach(((t,e)=>a[e].push(t)));return Object(E.m)(a.map((t=>w.Ce(t,0))))}))}predict(t,e={}){const s=d(t);y(s,this.inputNames,this.feedInputShapes,!1);try{const t=null==e.batchSize?32:e.batchSize;return l(t),this.predictLoop(s,t)}finally{f(s,t)}}predictOnBatch(t){y(t,this.inputNames,this.feedInputShapes,!0);const e=(Array.isArray(t)?t[0]:t).shape[0];return this.predictLoop(t,e)}standardizeUserDataXY(t,e,s=!0,n){if(null==this.optimizer_)throw new $.d("You must compile a model before training/testing. Use LayersModel.compile(modelCompileArgs).");const i=[];for(let a=0;a<this.feedOutputShapes.length;++a){const t=this.feedOutputShapes[a];this.feedLossFns[a]===x.j?i.push(t.slice(0,t.length-1).concat([1])):i.push(t)}if(function(t,e,s){const n=Object(E.q)(t.map((t=>t.shape[0])));n.sort();const i=Object(E.q)(e.map((t=>t.shape[0])));if(i.sort(),n.length>1)throw new $.e("All input Tensors (x) should have the same number of samples. Got array shapes: "+JSON.stringify(t.map((t=>t.shape))));if(i.length>1)throw new $.e("All target Tensors (y) should have the same number of samples. Got array shapes: "+JSON.stringify(e.map((t=>t.shape))));if(n.length>0&&i.length>0&&!w.Si.arraysEqual(n,i))throw new $.e(`Input Tensors should have the same number of samples as target Tensors. Found ${n[0]} input sample(s) and ${i[0]} target sample(s).`)}(t=m(t,this.feedInputNames,this.feedInputShapes,!1,"input"),e=m(e,this.feedOutputNames,i,!1,"target")),function(t,e,s){const n=[x.i,x.a,x.b];for(let i=0;i<t.length;++i){const a=t[i],o=e[i],r=s[i];if(null!=o){if(o===x.b&&1===a.shape[a.shape.length-1])throw new $.e(`You are passing a target array of shape ${a.shape} while using a loss 'categorical_crossentropy'. 'categorical_crossentropy'expects targets to be binary matrices (1s and 0s) of shape [samples, classes].`);if(-1!==n.indexOf(o)){const t=a.shape.slice(1),e=r.slice(1);for(let s=0;s<t.length;++s){const n=t[s],i=e[s];if(null!=i&&n!==i)throw new $.e(`A target Tensor with shape ${a.shape} was passed for an output of shape ${r}, while using a loss function that expects targets to have the same shape as the output.`)}}}}}(e,this.feedLossFns,this.feedOutputShapes),this.stateful&&null!=n&&n>0&&t[0].shape[0]%n!=0)throw new $.e(`In a stateful network, you should only pass inputs with a number of samples that is divisible by the batch size ${n}. Found: ${t[0].shape[0]} sample(s).`);return[t,e]}async standardizeUserData(t,e,s,a,o=!0,r){const[h,l]=this.standardizeUserDataXY(t,e,o,r);if(null!=s)throw Error("sample weight is not supported yet.");let u=null;if(null!=a){const t=n(a,this.outputNames);u=[];for(let e=0;e<t.length;++e)u.push(await i(l[e],null,t[e]))}return[h,l,u]}testLoop(t,e,s,n=0,i){return w.Ei((()=>{const a=this.checkNumSamples(e,s,i,"steps"),o=[];if(n>0)throw new $.c("Verbose mode is not implemented yet.");if(null!=i)throw new $.c("steps mode in testLoop() is not implemented yet");{const n=p(a,s),i=Object(w.vi)(Object(j.e)(0,a));for(let s=0;s<n.length;++s){const a=n[s][0],r=n[s][1],h=S.q(i,a,r-a),l=c(e,h),u=t(l);if(0===s)for(let t=0;t<u.length;++t)o.push(Object(w.Fh)(0));for(let t=0;t<u.length;++t){const e=u[t];o[t]=w.Ud(o[t],w.Gg(r-a,e))}}for(let t=0;t<o.length;++t)o[t]=w.df(o[t],a)}return o}))}getDedupedMetricsNames(){const t=this.metricsNames,e=[];for(let s=0;s<t.length;++s){const n=t[s];let i=n;Object(E.e)(t,n)>1&&(i+="_"+Object(E.e)(t.slice(0,s),n)),e.push(i)}return e}makeTrainFunction(){return t=>{const e=[],s=t.slice(0,this.inputs.length),n=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),i=t.slice(this.inputs.length+this.outputs.length,this.inputs.length+2*this.outputs.length),o=[],r=this.collectedTrainableWeights.map((t=>t.read()));return[this.optimizer_.minimize((()=>{const t=[];for(let e=0;e<this.inputs.length;++e)t.push({key:this.inputs[e],value:s[e]});const r=new W.a(t),h=Object(W.b)(this.outputs,r,{training:!0});let l;for(let s=0;s<this.lossFunctions.length;++s){let t=(0,this.lossFunctions[s])(n[s],h[s]);null!=i[s]&&(t=a(t,i[s]));const o=w.xg(t);e.push(o),l=0===s?t:w.Ud(l,t)}for(let s=0;s<this.metricsTensors.length;++s){let t;if(this.outputs.length>1&&s<this.outputs.length)t=e[s];else{const e=this.metricsTensors[s][0],i=this.metricsTensors[s][1];t=w.xg(e(n[i],h[i]))}w.Xf(t),o.push(t)}return l=w.xg(l),this.calculateLosses().forEach((t=>{l=w.Ud(l,t)})),l}),!0,r)].concat(o)}}makeTestFunction(){this.testFunction=t=>w.Ei((()=>{const e=[];let s;const n=t.slice(0,this.inputs.length),i=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),a=[];for(let t=0;t<this.inputs.length;++t)a.push({key:this.inputs[t],value:n[t]});const o=new W.a(a),r=Object(W.b)(this.outputs,o);for(let t=0;t<this.lossFunctions.length;++t){const n=this.lossFunctions[t],a=w.xg(n(i[t],r[t]));s=0===t?a:w.Ud(s,a),e.push(s)}for(let t=0;t<this.metricsTensors.length;++t){const s=this.metricsTensors[t][0],n=this.metricsTensors[t][1],a=w.xg(s(i[n],r[n]));e.push(a)}return e}))}async fit(t,e,s={}){if(this.isTraining)throw Error("Cannot start training because another fit() call is ongoing.");let n,i,a,o,r,h,c,p,d;this.isTraining=!0;try{const f=null==s.batchSize?32:s.batchSize;l(f);const g=!1,b=await this.standardizeUserData(t,e,s.sampleWeight,s.classWeight,g,f);n=b[0],i=b[1],d=b[2];let m,y=!1;if(null!=s.validationData&&s.validationData.length>0){if(y=!0,2!==s.validationData.length)throw 3===s.validationData.length?new $.c("validationData including sample weights is not supported yet."):new $.e("When passing validation data, it must contain 2 (valX, valY) or 3 (valX, valY, valSampleWeight) items; "+s.validationData+" is invalid.");r=s.validationData[0],h=s.validationData[1];const t=!0,e=await this.standardizeUserData(r,h,null,null,t,f);c=e[0],p=e[1],m=c.concat(p)}else if(null!=s.validationSplit&&s.validationSplit>0&&s.validationSplit<1){y=!0;const t=Math.floor(n[0].shape[0]*(1-s.validationSplit)),e=n[0].shape[0];c=u(n,t,e),a=n,n=u(n,0,t),p=u(i,t,e),o=i,i=u(i,0,t),m=c.concat(p)}else null!=s.validationSteps&&(y=!0);const w=n.concat(i).concat(d);this.checkTrainableWeightsConsistency();const S=this.makeTrainFunction(),v=this.getDedupedMetricsNames();let T,N;y?(this.makeTestFunction(),T=this.testFunction,N=v.slice().concat(v.map((t=>"val_"+t)))):(T=null,m=[],N=v.slice());const x=Object(O.g)(s.callbacks,s.yieldEvery);return await this.fitLoop(S,w,v,f,s.epochs,s.verbose,x,T,m,s.shuffle,N,s.initialEpoch,null,null)}finally{this.isTraining=!1,f(n,t),f(i,e),f(a,t),f(o,e),f(c,r),f(p,h),null!=d&&w.bf(d)}}async fitLoop(t,e,s,n,i,a,o,r,h,l,u,d,f,g){null==n&&(n=32),null==i&&(i=1),null==l&&(l=!0),null==d&&(d=0);let b=!1;if(null!=r&&null!=h&&(b=!0),null!=g&&(b=!0,null==f))throw new $.e("Can only use `validationSteps` when doing step-wise training, i.e., `stepsPerEpoch` must be set.");const m=this.checkNumSamples(e,n,f,"steps_per_epoch");let y;null!=m&&(y=Object(j.e)(0,m)),null==a&&(a=1);const{callbackList:v,history:T}=Object(O.f)(o,a,i,d,m,f,n,b,u);v.setModel(this),this.history=T,await v.onTrainBegin(),this.stopTraining_=!1;for(let O=d;O<i;++O){await v.onEpochBegin(O);const i={};if(null!=f)throw new $.c("stepsPerEpoch mode is not implemented yet.");{if("batch"===l)throw new $.c("batch shuffling is not implemneted yet");l&&w.Si.shuffle(y);const a=Object(w.vi)(y),o=p(m,n);for(let l=0;l<o.length;++l){const u={};if(await v.onBatchBegin(l,u),w.Ei((()=>{const p=o[l][0],d=o[l][1],f=S.q(a,p,d-p);u.batch=l,u.size=d-p;const g=c(e,f),m=t(g);for(let t=0;t<s.length;++t){const e=s[t],n=m[t];u[e]=n,w.Xf(n)}if(l===o.length-1&&b){const t=this.testLoop(r,h,n);for(let e=0;e<s.length;++e){const n=s[e],a=t[e];w.Xf(a),i["val_"+n]=a}}})),await v.onBatchEnd(l,u),Object(N.a)(u),this.stopTraining_)break}a.dispose()}if(await v.onEpochEnd(O,i),this.stopTraining_)break}return await v.onTrainEnd(),await this.history.syncData(),this.history}async fitDataset(t,e){return async function(t,e,s){const a=null!=s.batchesPerEpoch;if(w.Si.assert(null!=t.optimizer,(()=>"You must compile a model before training/testing. Use LayersModel.compile(modelCompileConfig).")),w.Si.assert(null!=s,(()=>"For fitDataset(), the 2nd argument (config) is required, but it is not provided in this call.")),w.Si.assert(null!=s.epochs&&s.epochs>0&&Number.isInteger(s.epochs),(()=>"For fitDataset(), config.epochs is expected to be a positive integer, but got "+s.epochs)),w.Si.assert(!a||s.batchesPerEpoch>0&&Number.isInteger(s.batchesPerEpoch),(()=>"For fitDataset(), config.batchesPerEpoch is expected to be a positive integer if specified, but got "+s.batchesPerEpoch)),w.Si.assert(null==s.validationSplit,(()=>"`validationSplit` is not supported by `fitDataset()`. Use validationData instead.")),t.isTraining)throw Error("Cannot start training because another fit() call is ongoing.");t.isTraining=!0;try{const r=null!=s.validationData;let l,u;if(r)if(h(s.validationData))w.Si.assert(null==s.validationBatches||s.validationBatches>0&&Number.isInteger(s.validationBatches),(()=>"For fitDataset() with dataset-based validation, config.validationBatches is expected not to be provided, or to be a positive integer, but got "+s.validationBatches));else{const t=function(t){if(3===t.length)throw new $.c("Validation with sample weights is not implemented yet.");return{xs:t[0],ys:t[1]}}(s.validationData);l=t.xs,u=t.ys}const c=t.makeTrainFunction(),p=t.getDedupedMetricsNames();let d;d=r?p.slice().concat(p.map((t=>"val_"+t))):p.slice();const f=Object(O.g)(s.callbacks,s.yieldEvery),g=null==s.verbose?1:s.verbose,{callbackList:b,history:m}=Object(O.f)(f,g,s.epochs,null,null,function(t,e){let s=null;return null!=e.batchesPerEpoch?s=e.batchesPerEpoch:Number.isFinite(t.size)&&(s=t.size),s}(e,s),null,r,d);b.setModel(t),t.history=m,await b.onTrainBegin(),t.stopTraining_=!1;let y=null==s.initialEpoch?0:s.initialEpoch,S=await e.iterator();for(;y<s.epochs;){const d={};await b.onEpochBegin(y);let f=0,g=0;for(a||(S=await e.iterator());!a||f<s.batchesPerEpoch;){const e=await S.next();if(a&&e.done){console.warn("You provided `batchesPerEpoch` as "+s.batchesPerEpoch+", but your dataset iterator ran out of data after "+f+" batches; interrupting training. Make sure that your dataset can generate at least `batchesPerEpoch * epochs` batches (in this case, "+s.batchesPerEpoch*s.epochs+" batches). You may need to use the repeat() function when building your dataset.");break}if(null!=e.value){const{xs:a,ys:r}=o(t,e.value),h={};h.batch=g,h.size=a[0].shape[0],await b.onBatchBegin(g,h);const l=[];if(null!=s.classWeight){const e=n(s.classWeight,t.outputNames);for(let t=0;t<e.length;++t)l.push(await i(r[t],null,e[t]))}const u=a.concat(r).concat(l),d=c(u);w.bf(u);for(let t=0;t<p.length;++t){const e=p[t],s=d[t];h[e]=s,w.Xf(s)}await b.onBatchEnd(g,h),Object(N.a)(h),g++,f++}if(a?f>=s.batchesPerEpoch:e.done){if(r){let e;e=h(s.validationData)?Object(E.o)(await t.evaluateDataset(s.validationData,{batches:s.validationBatches})):Object(E.o)(t.evaluate(l,u,{batchSize:null==s.validationBatchSize?32:s.validationBatchSize,verbose:0}));for(let s=0;s<t.metricsNames.length;++s)d["val_"+t.metricsNames[s]]=e[s]}break}if(t.stopTraining_)break}if(await b.onEpochEnd(y,d),y++,t.stopTraining_)break}return await b.onTrainEnd(),await t.history.syncData(),t.history}finally{t.isTraining=!1}}(this,t,e)}async trainOnBatch(t,e){const s=await this.standardizeUserData(t,e),n=s[0],i=s[1],a=this.makeTrainFunction()(n.concat(i)),o=[];for(const r of a){const t=await r.data();o.push(t[0])}return w.bf(a),f(s[0],t),f(s[1],e),Object(E.m)(o)}getNamedWeights(t){const e=[],s=null!=t&&t.trainableOnly,n=s?this.trainableWeights:this.weights,i=this.getWeights(s);for(let a=0;a<n.length;++a)s&&!n[a].trainable||e.push({name:n[a].originalName,tensor:i[a]});return e}set stopTraining(t){this.stopTraining_=t}get stopTraining(){return this.stopTraining_}get optimizer(){return this.optimizer_}set optimizer(t){this.optimizer_!==t&&(this.optimizer_=t,this.isOptimizerOwned=!1)}dispose(){const t=super.dispose();if(0===t.refCountAfterDispose&&null!=this.optimizer&&this.isOptimizerOwned){const e=w.yg().numTensors;this.optimizer_.dispose(),t.numDisposedVariables+=e-w.yg().numTensors}return t}getLossIdentifiers(){let t;if("string"==typeof this.loss)t=Object(E.p)(this.loss);else if(Array.isArray(this.loss)){for(const t of this.loss)if("string"!=typeof t)throw Error("Serialization of non-string loss is not supported.");t=this.loss.map((t=>Object(E.p)(t)))}else{const e=Object.keys(this.loss);t={};const s=this.loss;for(const n of e){if("string"!=typeof s[n])throw Error("Serialization of non-string loss is not supported.");t[n]=Object(E.p)(s[n])}}return t}getMetricIdentifiers(){if("string"==typeof this.metrics||"function"==typeof this.metrics)return[Object(E.p)(k.f(this.metrics))];if(Array.isArray(this.metrics))return this.metrics.map((t=>Object(E.p)(k.f(t))));{const t={};for(const e in this.metrics)t[e]=Object(E.p)(k.f(this.metrics[e]));return t}}getTrainingConfig(){return{loss:this.getLossIdentifiers(),metrics:this.getMetricIdentifiers(),optimizer_config:{class_name:this.optimizer.getClassName(),config:this.optimizer.getConfig()}}}loadTrainingConfig(t){if(null!=t.weighted_metrics)throw Error("Loading weight_metrics is not supported yet.");if(null!=t.loss_weights)throw Error("Loading loss_weights is not supported yet.");if(null!=t.sample_weight_mode)throw Error("Loading sample_weight_mode is not supported yet.");const e=Object(D.a)(t.optimizer_config),s=Object(T.a)(e);let n,i;if("string"==typeof t.loss)n=Object(E.n)(t.loss);else if(Array.isArray(t.loss))n=t.loss.map((t=>Object(E.n)(t)));else if(null!=t.loss){n={};for(const e in t.loss)n[e]=Object(E.n)(t.loss[e])}if(Array.isArray(t.metrics))i=t.metrics.map((t=>Object(E.n)(t)));else if(null!=t.metrics){i={};for(const e in t.metrics)i[e]=Object(E.n)(t.metrics[e])}this.compile({loss:n,metrics:i,optimizer:s})}async save(t,e){if("string"==typeof t){const e=w.Sf.getSaveHandlers(t);if(0===e.length)throw new $.e(`Cannot find any save handlers for URL '${t}'`);if(e.length>1)throw new $.e(`Found more than one (${e.length}) save handlers for URL '${t}'`);t=e[0]}if(null==t.save)throw new $.e("LayersModel.save() cannot proceed because the IOHandler provided does not have the `save` attribute defined.");const s=await w.Sf.encodeWeights(this.getNamedWeights(e)),n={modelTopology:this.toJSON(null,!1),format:"layers-model",generatedBy:"TensorFlow.js tfjs-layers v"+z.a,convertedBy:null};if(null!=e&&e.includeOptimizer&&null!=this.optimizer){n.trainingConfig=this.getTrainingConfig();const t="optimizer",{data:e,specs:i}=await w.Sf.encodeWeights(await this.optimizer.getWeights(),t);s.specs.push(...i),s.data=w.Sf.concatenateArrayBuffers([s.data,e])}if(null!=this.userDefinedMetadata){const t=!0;Object(A.a)(this.userDefinedMetadata,this.name,t),n.userDefinedMetadata=this.userDefinedMetadata}return n.weightData=s.data,n.weightSpecs=s.specs,t.save(n)}setUserDefinedMetadata(t){Object(A.a)(t,this.name),this.userDefinedMetadata=t}getUserDefinedMetadata(){return this.userDefinedMetadata}}C.className="Model",w.Lh.registerClass(C);class M extends C{}M.className="Functional",w.Lh.registerClass(M)}}]);